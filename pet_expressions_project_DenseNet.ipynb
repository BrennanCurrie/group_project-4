{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10172,"status":"ok","timestamp":1691439006759,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"e508faf6"},"outputs":[],"source":["# Import Dependencies\n","import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","from IPython.display import display, Image\n","from keras.layers import BatchNormalization\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.applications import ResNet50, DenseNet121, EfficientNetB0\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import matplotlib.image as mpimg\n","import os\n","from keras.utils import to_categorical"],"id":"e508faf6"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":666,"status":"ok","timestamp":1691439007408,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"_TPEdqWkAlKQ","outputId":"f1abe4d5-2abf-46ee-f9e7-a3a54a597dc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Set up Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"_TPEdqWkAlKQ"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1691439007408,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"yMkxM8-gBy8U","outputId":"cb9884ed-7387-436d-b451-11fc5df0eec7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/pet_expressions_data\n"]}],"source":["# Change Directory\n","%cd /content/drive/MyDrive/pet_expressions_data"],"id":"yMkxM8-gBy8U"},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1691439007409,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"oHd6VWxVCdMn"},"outputs":[],"source":["# Define the path to the AUGMENTED dataset folders\n","happy_folder = \"/content/drive/MyDrive/pet_expressions_data/happy\"\n","sad_folder = \"/content/drive/MyDrive/pet_expressions_data/sad\"\n","angry_folder = \"/content/drive/MyDrive/pet_expressions_data/angry\"\n","other_folder = \"/content/drive/MyDrive/pet_expressions_data/other\""],"id":"oHd6VWxVCdMn"},{"cell_type":"code","source":["# Define path to the ORGINAL dataset folders\n","# happy_folder = \"/content/drive/MyDrive/pet_expressions_data_original/happy\"\n","# sad_folder = \"/content/drive/MyDrive/pet_expressions_data_original/sad\"\n","# angry_folder = \"/content/drive/MyDrive/pet_expressions_data_original/angry\"\n","# other_folder = \"/content/drive/MyDrive/pet_expressions_data_original/other\"\n"],"metadata":{"id":"f_hXgf19Gqjh","executionInfo":{"status":"ok","timestamp":1691439007409,"user_tz":360,"elapsed":10,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"}}},"id":"f_hXgf19Gqjh","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1691439007409,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"U7P5XmOCRo28"},"outputs":[],"source":["# Check the number of pictures in each 'emotion' folder, ideally we would want this to be equal\n","# Define function to count picturues within a folder, takes path as input\n","\n","def count_pics(folder_path):\n","    picture_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n","    picture_count = 0\n","\n","    # Check if the provided folder path exists\n","    if not os.path.exists(folder_path):\n","        print(\"Folder path does not exist.\")\n","        return 0\n","\n","    # Get a list of files in the folder\n","    files = os.listdir(folder_path)\n","\n","    # Count pictures with valid extensions\n","    for file in files:\n","        _, extension = os.path.splitext(file)\n","        if extension.lower() in picture_extensions:\n","            picture_count += 1\n","\n","    return picture_count"],"id":"U7P5XmOCRo28"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1691439007731,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"7gn4cZDKUKJi","outputId":"191381e7-448d-49a1-ff5c-c06fb0c7fe2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of pictures in the HAPPY folder: 612\n"]}],"source":["# Happy\n","happy_picture_count = count_pics(happy_folder)\n","print(f\"Number of pictures in the HAPPY folder: {happy_picture_count}\")"],"id":"7gn4cZDKUKJi"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1691439007731,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"JYCbnDnMVHbI","outputId":"f2f33fe4-d831-4923-c8f9-dce88f2c1b3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of pictures in the SAD folder: 605\n"]}],"source":["# Sad\n","sad_picture_count = count_pics(sad_folder)\n","print(f\"Number of pictures in the SAD folder: {sad_picture_count}\")"],"id":"JYCbnDnMVHbI"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1691439007732,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"IZNVKYnhVHx2","outputId":"0bd6fd36-fe8b-4281-f5df-8bda683aa182"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of pictures in the ANGRY folder: 594\n"]}],"source":["# Angry\n","angry_picture_count = count_pics(angry_folder)\n","print(f\"Number of pictures in the ANGRY folder: {angry_picture_count}\")"],"id":"IZNVKYnhVHx2"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1691439007732,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"pPVPzNS_VIFy","outputId":"420610ad-1b55-4b81-ce2a-32cc67387ed0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of pictures in the  OTHER folder: 559\n"]}],"source":[" # Other\n","other_picture_count = count_pics(other_folder)\n","print(f\"Number of pictures in the  OTHER folder: {other_picture_count}\")"],"id":"pPVPzNS_VIFy"},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1691439007732,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"gHD-rY4OGUdC"},"outputs":[],"source":["# Function to load and preprocess images\n","def load_images_from_folder(folder):\n","    images = []\n","    for filename in os.listdir(folder):\n","        img = cv2.imread(os.path.join(folder,filename))\n","        if img is not None:\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","            img = cv2.resize(img, (48, 48))  # Resize to a fixed size for the model\n","            images.append(img)\n","    return images"],"id":"gHD-rY4OGUdC"},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":73604,"status":"ok","timestamp":1691439081333,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"c76b61aa"},"outputs":[],"source":["# Load images\n","happy_images = load_images_from_folder(happy_folder)\n","sad_images = load_images_from_folder(sad_folder)\n","angry_images = load_images_from_folder(angry_folder)\n","other_images = load_images_from_folder(other_folder)"],"id":"c76b61aa"},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1691439081334,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"paWwdzBVibZC"},"outputs":[],"source":["# Shuffle images in folder\n","import random\n","\n","random.shuffle(happy_images)\n","random.shuffle(sad_images)\n","random.shuffle(angry_images)\n","random.shuffle(other_images)"],"id":"paWwdzBVibZC"},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1691439081334,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"7Y5JhOxVEjK5"},"outputs":[],"source":["# Create labels for each emotion category folder\n","happy_labels = [0] * len(happy_images)\n","sad_labels = [1] * len(sad_images)\n","angry_labels = [2] * len(angry_images)\n","other_labels = [3] * len(other_images)"],"id":"7Y5JhOxVEjK5"},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1691439081335,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"fE_-U9oSHLmz"},"outputs":[],"source":["# Concatenate images and labels\n","x = np.array(happy_images + sad_images + angry_images + other_images)\n","y = np.array(happy_labels + sad_labels + angry_labels + other_labels)"],"id":"fE_-U9oSHLmz"},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1691439081335,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"Bggs4WygHL4K"},"outputs":[],"source":["# Normalize pixel values to range [0, 1]\n","x = x.astype('float32') / 255.0"],"id":"Bggs4WygHL4K"},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1691439081335,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"CF-FOR4KHSz1"},"outputs":[],"source":["# One-hot encode the labels\n","\n","from tensorflow.keras.utils import to_categorical\n","\n","y = to_categorical(y, 4)"],"id":"CF-FOR4KHSz1"},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1691439081336,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"yDNk4ib-HT_T"},"outputs":[],"source":["# Split the data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=100)"],"id":"yDNk4ib-HT_T"},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3904,"status":"ok","timestamp":1691439085227,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"tgyVFrupWu_A"},"outputs":[],"source":["# Data pre-processing - resizing images to specific shapes for model input\n","input_shape_densenet = (224, 224, 3)\n","\n","# Function to resize pictures to variable shapes defined above\n","def resize_pics(images, input_shape):\n","    resized_images = []\n","    for img in images:\n","        img_resized = cv2.resize(img, (input_shape[0], input_shape[1]))\n","        img_resized = np.expand_dims(img_resized, axis=-1)\n","        img_resized = np.repeat(img_resized, 3, axis=-1)  # Add three channels to convert grayscale to RGB\n","        resized_images.append(img_resized)\n","    return np.array(resized_images)\n","\n","# Utilize function above to resize data\n","x_train_resized_densenet = resize_pics(x_train, input_shape_densenet)"],"id":"tgyVFrupWu_A"},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9937,"status":"ok","timestamp":1691439095161,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"1hPltMvvXF7i"},"outputs":[],"source":["# Fit DenseNet Model\n","densenet_base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape_densenet)\n","densenet_base_model.trainable = False\n","#Setting layer.trainable to False moves all the layer's weights from trainable to non-trainable.\n","#This is called \"freezing\" the layer: the state of a frozen layer won't be updated during training"],"id":"1hPltMvvXF7i"},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1691439095162,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"bUIPDH_rXN21"},"outputs":[],"source":["# Add custom classification head to the DenseNet model\n","densenet_global_avg_pooling = GlobalAveragePooling2D()(densenet_base_model.output)\n","densenet_output = Dense(4, activation='softmax')(densenet_global_avg_pooling)\n","densenet_model = Model(inputs=densenet_base_model.input, outputs=densenet_output)"],"id":"bUIPDH_rXN21"},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691439095162,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"taY4WEMAXN6k"},"outputs":[],"source":["# Compile the DenseNet model\n","densenet_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"id":"taY4WEMAXN6k"},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691439095163,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"qUsi3wynXXeN"},"outputs":[],"source":["# Establish early stopping\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-7)"],"id":"qUsi3wynXXeN"},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691439095163,"user":{"displayName":"Brennan Currie","userId":"11818399872199193995"},"user_tz":360},"id":"lQSqcX1ycUKE"},"outputs":[],"source":["# Passing epochs through variable\n","######EPOCS WARNING: LONG RUN TIME\n","epochs=50"],"id":"lQSqcX1ycUKE"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FCVuwYsXXiH","outputId":"ff187b28-fc8a-49a5-a121-fca2ed53d2e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","30/30 [==============================] - 379s 12s/step - loss: 1.4774 - accuracy: 0.2897 - val_loss: 1.3299 - val_accuracy: 0.3645 - lr: 0.0010\n","Epoch 2/50\n","30/30 [==============================] - 377s 13s/step - loss: 1.2956 - accuracy: 0.3814 - val_loss: 1.2052 - val_accuracy: 0.4720 - lr: 0.0010\n","Epoch 3/50\n","30/30 [==============================] - 372s 12s/step - loss: 1.2094 - accuracy: 0.4575 - val_loss: 1.1380 - val_accuracy: 0.4766 - lr: 0.0010\n","Epoch 4/50\n","30/30 [==============================] - 378s 13s/step - loss: 1.1401 - accuracy: 0.5023 - val_loss: 1.1150 - val_accuracy: 0.5187 - lr: 0.0010\n","Epoch 5/50\n","29/30 [============================>.] - ETA: 11s - loss: 1.0963 - accuracy: 0.5339"]}],"source":["# Train the models on resized training data and validation split,monitoring the validation loss and using the early stopping and learning rate scheduling callbacks to stop when necessary.\n","######EPOCS WARNING: LONG RUN TIME\n","densenet_history = densenet_model.fit(x_train_resized_densenet, y_train, batch_size=64, epochs=epochs, validation_split=0.1,callbacks=[early_stopping, lr_scheduler])\n"],"id":"7FCVuwYsXXiH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3IHd5CtXkae"},"outputs":[],"source":["# Resize test images to the input shape required by each model\n","x_test_resized_densenet = resize_pics(x_test, input_shape_densenet)"],"id":"F3IHd5CtXkae"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQAnLceQXkey"},"outputs":[],"source":["# Evaluate the models on test data\n","densenet_loss, densenet_accuracy = densenet_model.evaluate(x_test_resized_densenet, y_test)\n","\n","print(\"DenseNet Test accuracy:\", densenet_accuracy)"],"id":"QQAnLceQXkey"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUNGmEt1WgpZ"},"outputs":[],"source":["# Create loss plot\n","import plotly.graph_objects as go\n","\n","epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n","\n","# Create the line plot\n","fig = go.Figure(data=go.Scatter(x=epochs,y=densenet_history.history['loss'], mode='lines+markers'))\n","\n","# Update layout\n","fig.update_layout(title='Loss Plot',\n","                  xaxis_title='Epochs',\n","                  yaxis_title='Loss')\n","\n","# Show the plot\n","fig.show()"],"id":"aUNGmEt1WgpZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"raH6KNHhcH7o"},"outputs":[],"source":["# Create accuracy plot\n","import plotly.graph_objects as go\n","\n","# Create the line plot\n","fig = go.Figure(data=go.Scatter(x=epochs,y=densenet_history.history['accuracy'], mode='lines+markers'))\n","\n","# Update layout\n","fig.update_layout(title='Accuracy Plot',\n","                  xaxis_title='Epochs',\n","                  yaxis_title='Accuracy')\n","\n","# Show the plot\n","fig.show()"],"id":"raH6KNHhcH7o"},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_PkgdYLXGDO"},"outputs":[],"source":["##################################################################################"],"id":"V_PkgdYLXGDO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8wnDM_Jg6hE"},"outputs":[],"source":["# Save the trained model\n","densenet_model.save(\"pet_expressions_model_DenseNet.h5\")"],"id":"G8wnDM_Jg6hE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9TWPC47I8WW"},"outputs":[],"source":["##################################################################################\n","#FROM HERE DOWN IS COPIED FROM OUR SEQUENTIAL MODEL NOTEBOOK FILE. IT MAY/MAYNOT WORK FOR DEPLOYING THIS MODEL?"],"id":"J9TWPC47I8WW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OI2oPULfH34N"},"outputs":[],"source":["# from keras.models import load_model\n","# # Load the saved model\n","# loaded_model = load_model('pet_expressions_model.h5')"],"id":"OI2oPULfH34N"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vApWzW3iIBOL"},"outputs":[],"source":["# # Function to load and preprocess images\n","# def load_images_from_folder(folder):\n","#     images = []\n","#     for filename in os.listdir(folder):\n","#         img = cv2.imread(os.path.join(folder, filename))\n","#         if img is not None:\n","#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","#             img = cv2.resize(img, (48, 48))  # Resize to a fixed size for the model\n","#             images.append(img)\n","#     return images"],"id":"vApWzW3iIBOL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2hJ_t8uIBXv"},"outputs":[],"source":["# Load a custom test image\n","# custom_test_image_path = \"\"\n","\n","# custom_test_image = cv2.imread(custom_test_image_path)\n","# custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n","# custom_test_image = cv2.resize(custom_test_image, (48, 48))\n","# custom_test_image = custom_test_image.astype('float32') / 255.0"],"id":"G2hJ_t8uIBXv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoagi0XAIBhH"},"outputs":[],"source":["# # Reshape the image to match the model input shape\n","# custom_test_image = np.expand_dims(custom_test_image, axis=0)\n","# custom_test_image = np.expand_dims(custom_test_image, axis=-1)"],"id":"zoagi0XAIBhH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"r39cICOXIBqP"},"outputs":[],"source":["# # Make predictions on the custom test image\n","# prediction = loaded_model.predict(custom_test_image)\n","# prediction_prob = prediction[0]"],"id":"r39cICOXIBqP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QkEWBRMIBzy"},"outputs":[],"source":["# emotion_label = np.argmax(prediction[0])"],"id":"4QkEWBRMIBzy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiimcfpxH4CD"},"outputs":[],"source":["# # Map the predicted label to emotion class\n","# emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry', 3: 'feeling some type of way...'}\n","# predicted_emotion = emotion_classes[emotion_label]"],"id":"CiimcfpxH4CD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MF1XmXXHImD9"},"outputs":[],"source":["# # Print the custom test image and its predicted label\n","# print(f\"Predicted Emotion: {predicted_emotion}\")\n","# print(f\"Confidence [happy, sad, angry,feeling some type of way...]: {prediction_prob}\")"],"id":"MF1XmXXHImD9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"25M2Dw8nImNk"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# #Display the custom test image using matplotlib\n","# plt.imshow(custom_test_image[0, :, :, 0])\n","# plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n","# plt.axis('off')  # Hide axes\n","# plt.show()"],"id":"25M2Dw8nImNk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgZQk-xoImVE"},"outputs":[],"source":["# from PIL import Image\n","# # Display the original custom test image using PIL\n","# img_pil = Image.open(custom_test_image_path)\n","# plt.imshow(np.array(img_pil))\n","# plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n","# plt.axis('off')  # Hide axes\n","# plt.show()"],"id":"VgZQk-xoImVE"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}